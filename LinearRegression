//
// Created by john on 2018-10-16.
//

#ifndef NEW_ML_CPP_SIMP_LINEAR_REG_H
#define NEW_ML_CPP_SIMP_LINEAR_REG_H



#include "../Numpy_Cpp/Numpy_Cpp.h"
#include <iostream>

using namespace std;

template<typename T>
T mse_test(MinVector<T> y_predict, MinVector<T> Y_test) {
    Numpy_Cpp<T> np;
    auto _mse = (y_predict - Y_test)._pow();
    return np.sum(_mse) / Y_test._size();
}

template<typename T>
T rmse_test(MinVector<T> y_predict, MinVector<T> Y_test) {
    return sqrt(mse_test(y_predict, Y_test));
}

template<typename T>
T mae_test(MinVector<T> y_predict, MinVector<T> Y_test) {
    Numpy_Cpp<T> np;
    auto _mae = y_predict.__abs_sub__(Y_test);
    return np.sum(_mae)/Y_test._size() ;
}

template<typename T>
T r2_score(MinVector<T> y_predict, MinVector<T> Y_test) {
    Numpy_Cpp<T> np;
    return 1 - mse_test(y_predict, Y_test) /  np.var(Y_test);
}


template <typename T>
class sim_Linear_Reg{
private:
    MinVector<T> x;
    MinVector<T> y;

public:
    T a,b;

public:
    sim_Linear_Reg(MinVector<T> x,MinVector<T> y):x(x),y(y){

    }

    sim_Linear_Reg fit() {
        Numpy_Cpp<T> np;

        auto x_mean = np.mean(x);
        auto y_mean = np.mean(y);

        T num = 0.0;
        T d = 0.0;

        for (int i = 0; i < x._size(); ++i) {
            num += (x[i] - x_mean) * (y[i] - y_mean);
            d += pow((x[i] - x_mean), 2);
        }

        this->a = num / d;

        this->b = y_mean - a * x_mean;

        return *this;
    }

    MinVector<T> predict(MinVector<T> x_predict) {
        vector<T> array(x_predict._size());
        auto size = x_predict._size();
        for (int i = 0; i < size; ++i) {
            array[i] = this->_predict(x_predict[i]);
        }

        return MinVector<T>(array);
    }

    T score(MinVector<T> x_test, MinVector<T> y_test) {
        auto y_predict = this->predict(x_test);
        return r2_score(y_predict, y_test);
    }

private:
    T _predict(T x_single) {
        return this->a * x_single + this->b;
    }
};


template <typename T>
class LinearRegression{
public:
    MinVector<T> coef;
    T interception_;
private:
    MinVector<T> _theta;
    Numpy_Cpp<T> np;

public:
    LinearRegression() {

    }

    LinearRegression fit_normal(MinMatrix<T> X_train, MinVector<T> y_train) {
        assert(X_train.size() == y_train._size());

        auto X_b = MinMatrix<T>().ones(X_train.size(),
                                               X_train.col_num() + 1);

        auto col_size = X_train.col_num();
        for (int i = 0; i < col_size; ++i) {
            auto vec = X_train.col_vector(i);
            X_b.col_value_Change(i + 1, vec);
        }

        auto one = MinMatrix<T>(1,y_train)._T();

        auto X_b_T_temp = X_b._T().dot(X_b);
        auto X_b_T = X_b._T();
        X_b = inv(X_b_T_temp).dot(X_b_T).dot(one);

        int num = 0;

        this->_theta = X_b.col_vector(num);

        this->interception_ = this->_theta[0];

        this->coef = this->_theta._slice_vec(1, this->_theta._size());

        return *this;
    }

    MinVector<T> predict(MinMatrix<T> X_predict) {
        assert(X_predict.shape()[1] == this->coef._size());

        MinMatrix<T> X_b = MinMatrix<T>().ones(X_predict.size(),
                                               X_predict.col_num() + 1);
        auto col_size = X_predict.col_num();
        for (int i = 0; i < col_size; ++i) {
            auto vec = X_predict.col_vector(i);
            X_b.col_value_Change(i + 1,vec );
        }

        auto t = MinMatrix<T>(this->_theta._size(), this->_theta)._T();

        int num = 0;

        return X_b.dot(t).col_vector(num);

    }

    T score(MinMatrix<T> x_test, MinVector<T> y_test) {
        auto y_predict = this->predict(x_test);
        return r2_score(y_predict, y_test);

    }

    //梯度下降
    T J(MinVector<T> theta , MinMatrix<T> X_b , MinVector<T> y){
        try{
            return np.sum((y - X_b.dot(theta))._pow()) / X_b.size();
        }catch (exception e){
            return  1.0/0.0;
        }
    }

    MinVector<T> dJ(MinVector<T> &theta , MinMatrix<T> &X_b , MinVector<T> &y){
        //vector<T> res(theta._vector().size());
        //res[0] = np.sum(X_b.dot(theta) - y);
        //for( int i = 1 ; i < theta._vector().size(); ++i){
        //    res[i] = (X_b.dot(theta) - y).dot(X_b.col_vector(i));
        //}
        //auto ret = MinVector<T>(res) * (2.0/y._size());
        //return ret;
        auto ret = X_b._T().dot(X_b.dot(theta)- y) * (2./y._size());
        return ret;
    }

    MinVector<T> gra_des(MinMatrix<T> &X_b,MinVector<T> &y,MinVector<T> &int_theta,T eta,T n_ites = 1e4,T epslion = 1e-8){
        auto theta = int_theta;
        auto cur = 0;
        while(cur < n_ites){

            auto grad = dJ(theta,X_b,y);
            auto l_the = theta;
            theta = theta - grad * eta;

            if(abs(J(theta,X_b,y) - J(l_the,X_b,y))< epslion){
                break;
            }
            cur++;
        }
        return theta;
    }

    LinearRegression fit_gd(MinMatrix<T> &X_train,MinVector<T> &Y_train,T eta = 0.01 ,T n_iters = 1e4){
        assert(X_train.shape()[0] == Y_train._size());
        auto X_b = np.hstack(MinMatrix<double>().ones(static_cast<int &&>(X_train.size()), 1), X_train);

        auto init_the = MinVector<double>().zero(static_cast<int &&>(X_b.shape()[1]));
        this->_theta = gra_des(X_b,Y_train,init_the,eta);
        this->interception_ = this->_theta[0];
        this->coef = this->_theta._slice_vec(1,this->_theta._size());
        return *this;
    }
};



#endif //NEW_ML_CPP_SIMP_LINEAR_REG_H
